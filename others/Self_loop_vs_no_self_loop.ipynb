{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Self loop vs. no self loop.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cVr6wNeeUvD"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from __future__ import print_function, division\n",
        "import jax.numpy as np\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random\n",
        "import numpy as onp\n",
        "import time as time_time\n",
        "# # JAX is a package, by the same author of the paper, for speeding up \n",
        "# # linear algebra-heavy operations"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rusIAYhJeVv2"
      },
      "source": [
        "# Generate randomness\n",
        "\n",
        "def keygen(key, nkeys):\n",
        "  \"\"\"Generate randomness that JAX can use by splitting the JAX keys.\n",
        "  Args:\n",
        "    key : the random.PRNGKey for JAX\n",
        "    nkeys : how many keys in key generator\n",
        "  Returns:\n",
        "    2-tuple (new key for further generators, key generator)\n",
        "  \"\"\"\n",
        "  keys = random.split(key, nkeys+1)\n",
        "  return keys[0], (k for k in keys[1:])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLY78EMGgrfk"
      },
      "source": [
        "# Generate random parameters \n",
        "\n",
        "def random_esn_params(key, u, n, m, tau=1.0, dt=0.1, g=1.0):\n",
        "  \"\"\"Generate random RNN parameters\n",
        "  \n",
        "  Arguments: \n",
        "    u: dim of the input\n",
        "    n: dim of the hidden state\n",
        "    m: dim of the output\n",
        "    tau: \"neuronal\" time constant\n",
        "    dt: time between Euler integration updates\n",
        "    g: scaling of the recurrent matrix in the reservoir\n",
        "\n",
        "  Returns:\n",
        "    A dictionary of parameters for the ESN.\n",
        "  \"\"\"\n",
        "\n",
        "  key, skeys = keygen(key, 5)\n",
        "  hscale = 0.25\n",
        "  ifactor = 1.0 / np.sqrt(u)\n",
        "  hfactor = g / np.sqrt(n)\n",
        "  pfactor = 1.0 / np.sqrt(n)\n",
        "  ffactor = 1.0 # Feedback factor, keep at 1 for now.\n",
        "  return {'a0' : random.normal(next(skeys), (n,)) * hscale,\n",
        "          'wI' : random.normal(next(skeys), (n,u)) * ifactor,\n",
        "          'wR' : random.normal(next(skeys), (n,n)) * hfactor,\n",
        "          'wO' : random.normal(next(skeys), (m,n)) * pfactor,\n",
        "          'wF' : random.normal(next(skeys), (n,m)) * ffactor,\n",
        "          'dt_over_tau' : dt / tau}\n",
        "\n",
        "\n",
        "def new_force_params(n, alpha=1.0):\n",
        "  \"\"\"Generate new 'parameters' for the RLS learning rule.\n",
        "\n",
        "  This routine essentially initializes the inverse correlation matrix in RLS.\n",
        "\n",
        "  Arguments:\n",
        "    n: dim of hidden state\n",
        "    alpha: initial learning rate\n",
        "\n",
        "  Returns: \n",
        "    A dictionary with RLS parameters.\n",
        "  \"\"\"\n",
        "\n",
        "  identity_3d = onp.zeros((n,n,n))\n",
        "  idx = onp.arange(n)\n",
        "  identity_3d[:, idx, idx] = 1    \n",
        "\n",
        "  return {'P' : np.eye(n) * alpha, 'P_recurr': np.array(identity_3d)*alpha}\n",
        "\n",
        "\n",
        "# Run one iteration of \"forward pass\". \n",
        "# TODO: Implement in Keras model\n",
        "def esn(x, a, h, z, wI, wR, wF, wO, dtdivtau):\n",
        "  \"\"\"Run the continuous-time Echostate network one step.\n",
        "  \n",
        "    da/dt = -a + wI x + wR h + wF z\n",
        "\n",
        "    Arguments:\n",
        "      x: ndarray of input to ESN\n",
        "      a: ndarray of activations (pre nonlinearity) from prev time step\n",
        "      h: ndarray of hidden states from prev time step\n",
        "      z: ndarray of output from prev time step\n",
        "      wI: ndarray, input matrix, shape (n, u)\n",
        "      wR: ndarray, recurrent matrix, shape (n, n)\n",
        "      wF: ndarray, feedback matrix, shape (n, m)\n",
        "      wO: ndarray, output matrix, shape (m, n)\n",
        "      dtdivtau: dt / tau\n",
        "\n",
        "    Returns: \n",
        "      The update to the ESN at this time step.\n",
        "  \"\"\"\n",
        "  dadt = -a + np.dot(wI, x) + np.dot(wR, h) # + np.dot(wF, z)\n",
        "  a = a + dtdivtau * dadt\n",
        "  h = np.tanh(a)\n",
        "  z = np.dot(wO, h)\n",
        "  \n",
        "  return a, h, z\n",
        "\n",
        "# Calculate weight updates\n",
        "# TODO: Implement in Keras model\n",
        "def rls(h, z, f, wO, P):\n",
        "  \"\"\"Perform the recursive least squares step.\n",
        "  \n",
        "    Arguments: \n",
        "      h: ndarray of hidden state at current time step\n",
        "      z: ndarray of output at current time step\n",
        "      f: ndarray of targets at current time step\n",
        "      wO: ndarray of output weights, shape (m, n)\n",
        "      P: ndarray, inverse correlation matrix, shape (n,n)\n",
        "\n",
        "    Returns: \n",
        "      A 2-tuple of the updated wO, and updated P\n",
        "  \n",
        "  \"\"\"\n",
        "  # See paper for meanings of P and w\n",
        "  # update inverse correlation matrix\n",
        "  # k = np.expand_dims(np.dot(P, h), axis=1)\n",
        "  # hPh = np.dot(h.T, k)\n",
        "  # c = 1.0/(1.0 + hPh)\n",
        "  # # print(k.shape)\n",
        "  # # print(hPh.shape)\n",
        "  # # print(c.shape)\n",
        "  # # print()\n",
        "  \n",
        "  # P = P - np.dot(k*c, k.T)\n",
        "    \n",
        "  # # update the output weights\n",
        "  # e = np.atleast_2d(z-f)\n",
        "  # dw = np.dot(-c*k, e).T\n",
        "  # # print(dw.shape)\n",
        "  # print(e.shape)\n",
        "\n",
        "  k = np.expand_dims(np.dot(P, h), axis=1)\n",
        "  hPh = np.dot(h.T, k)\n",
        "  c = 1.0/(1.0+hPh)\n",
        "\n",
        "  dP = np.dot(c*k, np.transpose(k))\n",
        "  P = P - dP\n",
        "\n",
        "  e = np.atleast_2d(z-f)\n",
        "  dw = np.dot(np.expand_dims(np.dot(P, h), axis=1), e).T\n",
        "  return wO - dw, P, dw, dP     \n",
        "\n",
        "\n",
        "def update_recurr_P(h, z, f, P_recurr, wR):\n",
        "  # zero out the rows and columsn of P where the weights are 0\n",
        "  # I,J = np.nonzero(wR==0)\n",
        "  # P_recurr[I,:,J]=0\n",
        "  # P_recurr[I,J,:]=0\n",
        "\n",
        "  h = np.expand_dims(h,axis = 1)\n",
        "  Ph = np.dot(P_recurr, h)[:,:,0] # need to multiply by error term to get n x n matrix of weight updates (indiced by i x j)\n",
        "\n",
        "  hPh = np.expand_dims(np.dot(Ph, h),axis = 2) # n x 1 x 1 array for i\n",
        "\n",
        "  #htP = np.dot(np.transpose(h),P_recurr)[0] # indiced by i x k, n x n matrix \n",
        "  \n",
        "  dP_recurr =  np.expand_dims(Ph, axis = 2) * np.expand_dims(Ph, axis = 1)  / (1+hPh) # Ph[:,:,None]*htP[:,None,:] / (1+hPh)\n",
        "\n",
        "  P_recurr -= dP_recurr\n",
        "\n",
        "  e = np.atleast_2d(z-f)\n",
        "  assert e.shape == (1,1)\n",
        "\n",
        "  dwR = e*np.dot(P_recurr, h)[:,:,0]\n",
        "\n",
        "  return wR - dwR, P_recurr, dwR, dP_recurr \n",
        "  #h_mask = wR_mask * h\n",
        "  #dwR = np.diagonal(np.dot(P_recurr, h_mask), axis1=0, axis2=2) # missing error term\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDP7zhDPgsuq"
      },
      "source": [
        "# This shows the beginning to end pipeline\n",
        "# This should all change in the Keras implementation\n",
        "# For a first pass, ignore the use of JAX and try to get it working\n",
        "# with no optimizations (i.e. only using numpy/tensorflow)\n",
        "\n",
        "def esn_run_and_train_jax(params, fparams, x_t, f_t=None, do_train=False):\n",
        "  \"\"\"Run the Echostate network forward a number of steps the length of x_t.\n",
        "  \n",
        "    This implementation uses JAX to build the outer time loop from basic\n",
        "    Python for loop.\n",
        "\n",
        "    Arguments: \n",
        "      params: dict of ESN params\n",
        "      fparams: dict of RLS params\n",
        "      x_t: ndarray of input time series, shape (t, u)\n",
        "      f_t: ndarray of target time series, shape (t, m)\n",
        "      do_train: Should the network be trained on this run? \n",
        "    \n",
        "    Returns:\n",
        "      4-tuple of params, fparams, h_t, z_t, after running ESN and potentially\n",
        "        updating the readout vector.  \n",
        "  \"\"\"\n",
        "  # per-example predictions\n",
        "  a = params['a0']\n",
        "  h = np.tanh(a)\n",
        "  wO = params['wO']\n",
        "  wI = params['wI']\n",
        "  wR = params['wR']\n",
        "  wF = params['wF']\n",
        "  z = np.dot(wO, h)\n",
        "  if do_train:\n",
        "    P = fparams['P']\n",
        "    P_recurr = fparams['P_recurr']\n",
        "    # I,J = onp.nonzero(onp.array(wR)==0)\n",
        "    # P_recurr[I,:,J]=0\n",
        "    # P_recurr[I,J,:]=0\n",
        "    # P_recurr=np.array(P_recurr)\n",
        "  else:\n",
        "    P = None\n",
        "    P_recurr = None\n",
        "  h_t = []\n",
        "  z_t = []\n",
        "\n",
        "  dP = 0\n",
        "  dw = 0\n",
        "\n",
        "  dtdivtau = params['dt_over_tau']\n",
        "  for tidx, x in enumerate(x_t):\n",
        "    a, h, z = esn(x, a, h, z, wI, wR, wF, wO, dtdivtau)\n",
        "\n",
        "    if do_train:\n",
        "      wO, P, dw, dP = rls(h, z, f_t[tidx], wO, P)\n",
        "      wR, P_recurr, dwR, dP_recurr = update_recurr_P(h, z, f_t[tidx], P_recurr, wR)\n",
        "    h_t.append(h)\n",
        "    z_t.append(z)\n",
        "    \n",
        "  if do_train:\n",
        "    fparams['P'] = P\n",
        "    fparams['P_recurr'] = P_recurr\n",
        "  params['wO'] = wO\n",
        "  params['wR'] = wR\n",
        "  params['a0'] = a\n",
        "  \n",
        "  h_t = np.array(h_t)  \n",
        "  z_t = np.array(z_t)\n",
        "  return params, fparams, h_t, z_t\n",
        "\n",
        "\n",
        "def esn_run_jax(params, x_t):\n",
        "  \"\"\"Run the echostate network forward.\n",
        "\n",
        "    Arguments:\n",
        "      params: dict of ESN params\n",
        "      x_t: ndarray of input with shape (t,u)\n",
        "\n",
        "    Returns: \n",
        "      2-tuple of ndarrays with first dim time, the hidden state and the outputs.\n",
        "  \"\"\"\n",
        "  _, _, h_t, z_t  = esn_run_and_train_jax(params, None, x_t, \n",
        "                                          None, do_train=False)\n",
        "  return h_t, z_t\n",
        "\n",
        "esn_run_jax_jit = jit(esn_run_jax)\n",
        "\n",
        "\n",
        "def esn_train_jax(params, fparams, x_t, f_t):\n",
        "  \"\"\"Run the echostate network forward and also train it.\n",
        "\n",
        "    Arguments:\n",
        "      params: dict of ESN params\n",
        "      fparams: dict of RLS params\n",
        "      x_t: ndarray of inputs with shape (t,u)\n",
        "      f_t: ndarray of targets with shape (t,m)\n",
        "\n",
        "    Returns: \n",
        "      4-tuple of updated params, fparams, and also ndarrays with first dim \n",
        "        time, the hidden state and the outputs.\n",
        "  \"\"\"\n",
        "  return esn_run_and_train_jax(params, fparams, x_t, f_t, do_train=True)\n",
        "  \n",
        "esn_train_jax_jit = jit(esn_train_jax)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ustA9m1sQEEy",
        "outputId": "688b2225-2d83-4616-9a8f-356780402d62"
      },
      "source": [
        "# Basic parameters of the Echostate networks\n",
        "key = random.PRNGKey(0)\n",
        "\n",
        "T = 30              # total time\n",
        "u = 1               # number of inputs (didn't bother to set up zero, just put in zeros)\n",
        "n = 400            # size of the reservoir in the ESN\n",
        "tau = 1          # neuron time constant\n",
        "dt = tau / 10.0     # Euler integration step\n",
        "time = np.arange(0, T, dt) # all time\n",
        "ntime = time.shape[0]      # the number of time steps\n",
        "alpha = 1e0\n",
        "m = 1\n",
        "\n",
        "x_t = np.zeros((ntime,u)) # Just a stand-in in folks want a real input later"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-t8LJbg-gyFI",
        "outputId": "ceaef9db-eb2a-4824-cd2e-3a7cf3b0d7e6"
      },
      "source": [
        "# Generate some target data by running an ESN, and just grabbing hidden \n",
        "# dimensions as the targets of the FORCE trained network.\n",
        "\n",
        "g = 1.8  # Recurrent scaling of the data ESN, gives how wild the dynamics are.\n",
        "\n",
        "data_seed = onp.random.randint(0, 10000000)\n",
        "print(\"Data seed: %d\" % (data_seed))\n",
        "key = random.PRNGKey(data_seed)\n",
        "data_params = random_esn_params(key, u, n, m, g=g)\n",
        "h_t, z_t = esn_run_jax_jit(data_params, x_t)\n",
        "\n",
        "f_t = h_t[:,0:m] # This will be the training data for the trained ESN\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data seed: 8717853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vF252auev78",
        "outputId": "06d0ed8f-10c2-401e-d6e9-89ee671d5986"
      },
      "source": [
        "g = 1.5\n",
        "params_seed = onp.random.randint(0, 10000000)\n",
        "print(\"Params seed %d\" %(params_seed))\n",
        "key = random.PRNGKey(params_seed)\n",
        "init_params = random_esn_params(key, u, n, m, g=g)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params seed 3836463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO5p-pUN7Mhh"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import backend, activations"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl8a6wgXYzSR"
      },
      "source": [
        "wI = tf.transpose(tf.convert_to_tensor(init_params['wI']))\n",
        "wR = tf.transpose(tf.convert_to_tensor(init_params['wR']))\n",
        "wF = tf.transpose(tf.convert_to_tensor(init_params['wF']))\n",
        "wO = tf.transpose(tf.convert_to_tensor(init_params['wO']))\n",
        "\n",
        "a0 = tf.convert_to_tensor(tf.expand_dims(init_params['a0'], axis = 0))\n",
        "\n",
        "f_t =  tf.convert_to_tensor(f_t,dtype=tf.float32) \n",
        "input2 = tf.convert_to_tensor(x_t)\n",
        "ntraining = 20"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G7jAkKMMHgF"
      },
      "source": [
        "class FORCELayer(keras.layers.AbstractRNNCell):\n",
        "    def __init__(self, units, output_size, activation, seed = None, g = 1.5, \n",
        "                 input_kernel_trainable = False, recurrent_kernel_trainable = False, \n",
        "                 output_kernel_trainable = True, feedback_kernel_trainable = False, p_recurr = 1, **kwargs):\n",
        "                \n",
        "        self.units = units \n",
        "        self._output_size = output_size\n",
        "        self.activation = activations.get(activation)\n",
        "\n",
        "        if seed is None:\n",
        "          self.seed_gen = tf.random.Generator.from_non_deterministic_state()\n",
        "        else:\n",
        "          self.seed_gen = tf.random.Generator.from_seed(seed)\n",
        "        \n",
        "        self._g = g\n",
        "\n",
        "        self._input_kernel_trainable = input_kernel_trainable\n",
        "        self._recurrent_kernel_trainable = recurrent_kernel_trainable\n",
        "        self._feedback_kernel_trainable = feedback_kernel_trainable\n",
        "        self._output_kernel_trainable = output_kernel_trainable\n",
        "        self._p_recurr = p_recurr\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    @property\n",
        "    def state_size(self):\n",
        "        return [self.units, self.units, self.output_size]\n",
        "\n",
        "    @property \n",
        "    def output_size(self):\n",
        "        return self._output_size\n",
        "\n",
        "    def initialize_input_kernel(self, input_shape, input_kernel = None):\n",
        "        if input_kernel is None:\n",
        "            initializer = keras.initializers.RandomNormal(mean=0., \n",
        "                                                          stddev= 1/input_shape**0.5, \n",
        "                                                          seed=self.seed_gen.uniform([1], \n",
        "                                                                                    minval=None, \n",
        "                                                                                    dtype=tf.dtypes.int64)[0])\n",
        "            input_kernel = initializer(shape = (input_shape, self.units))\n",
        "         \n",
        "        self.input_kernel = self.add_weight(shape=(input_shape, self.units),\n",
        "                                            initializer=keras.initializers.constant(input_kernel),\n",
        "                                            trainable = self._input_kernel_trainable,\n",
        "                                            name='input_kernel')\n",
        "        \n",
        "    def initialize_recurrent_kernel(self, recurrent_kernel = None):\n",
        "        if recurrent_kernel is None:        \n",
        "            initializer = keras.initializers.RandomNormal(mean=0., \n",
        "                                                          stddev= self._g/self.units**0.5, \n",
        "                                                          seed=self.seed_gen.uniform([1], \n",
        "                                                                                      minval=None, \n",
        "                                                                                      dtype=tf.dtypes.int64)[0])\n",
        "        \n",
        "            recurrent_kernel = self._p_recurr*keras.layers.Dropout(1-self._p_recurr)(initializer(shape = (self.units, self.units)), \n",
        "                                                                                    training = True)\n",
        "\n",
        "        self.recurrent_kernel = self.add_weight(shape=(self.units, self.units),\n",
        "                                                initializer=keras.initializers.constant(recurrent_kernel),\n",
        "                                                trainable = self._recurrent_kernel_trainable,\n",
        "                                                name='recurrent_kernel')\n",
        "    \n",
        "    def initialize_feedback_kernel(self, feedback_kernel = None):\n",
        "        if feedback_kernel is None:\n",
        "            initializer = keras.initializers.RandomNormal(mean=0., \n",
        "                                                          stddev= 1, \n",
        "                                                          seed=self.seed_gen.uniform([1], \n",
        "                                                                                 minval=None, \n",
        "                                                                                 dtype=tf.dtypes.int64)[0])\n",
        "            feedback_kernel = initializer(shape = (self.output_size, self.units))\n",
        "\n",
        "        self.feedback_kernel = self.add_weight(shape=(self.output_size, self.units),\n",
        "                                                initializer=keras.initializers.constant(feedback_kernel),\n",
        "                                                trainable = self._feedback_kernel_trainable,\n",
        "                                                name='feedback_kernel')\n",
        "                                            \n",
        "\n",
        "    def initialize_output_kernel(self, output_kernel = None):\n",
        "        if output_kernel is None:\n",
        "            initializer=keras.initializers.RandomNormal(mean=0., \n",
        "                                                        stddev= 1/self.units**0.5, \n",
        "                                                        seed=self.seed_gen.uniform([1], \n",
        "                                                                                   minval=None, \n",
        "                                                                                   dtype=tf.dtypes.int64)[0])\n",
        "            output_kernel = initializer(shape = (self.units, self.output_size))\n",
        "\n",
        "        self.output_kernel = self.add_weight(shape=(self.units, self.output_size),\n",
        "                                              initializer=keras.initializers.constant(output_kernel),\n",
        "                                              trainable = self._output_kernel_trainable,\n",
        "                                              name='output_kernel')     \n",
        "    \n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.initialize_input_kernel(input_shape[-1])\n",
        "        self.initialize_recurrent_kernel()\n",
        "        self.initialize_feedback_kernel()\n",
        "        self.initialize_output_kernel() \n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    @classmethod\n",
        "    def from_weights(cls, weights, **kwargs):\n",
        "        # Initialize the network from a list of weights (e.g., user-generated)\n",
        "        input_kernel, recurrent_kernel, feedback_kernel, output_kernel = weights \n",
        "        input_shape, input_units = input_kernel.shape \n",
        "        recurrent_units1, recurrent_units2 = recurrent_kernel.shape \n",
        "        feedback_output_size, feedback_units = feedback_kernel.shape \n",
        "        output_units, output_size = output_kernel.shape \n",
        "\n",
        "\n",
        "        units = input_units \n",
        "        assert np.all(np.array([input_units, recurrent_units1, recurrent_units2, \n",
        "                            feedback_units, output_units]) == units)\n",
        "\n",
        "        assert feedback_output_size == output_size \n",
        "        assert 'p_recurr' not in kwargs.keys(), 'p_recurr not supported in this method'\n",
        "\n",
        "        self = cls(units=units, output_size=output_size, p_recurr = None, **kwargs)\n",
        "\n",
        "        self.initialize_input_kernel(input_shape, input_kernel)\n",
        "        self.initialize_recurrent_kernel(recurrent_kernel)\n",
        "        self.initialize_feedback_kernel(feedback_kernel)\n",
        "        self.initialize_output_kernel(output_kernel)\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "        return self\n",
        "\n",
        "class FORCEModel(keras.Model):\n",
        "    def __init__(self, force_layer, alpha_P=1.,return_sequences=True):\n",
        "        super().__init__()\n",
        "        self.alpha_P = alpha_P\n",
        "        self.force_layer = keras.layers.RNN(force_layer, \n",
        "                                            stateful=True, \n",
        "                                            return_state=True, \n",
        "                                            return_sequences=return_sequences)\n",
        "\n",
        "        self.units = force_layer.units \n",
        "\n",
        "        self.original_force_layer = force_layer\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super().build(input_shape)\n",
        "        self.initialize_P()\n",
        "        self.initialize_train_idx()\n",
        "\n",
        "    def initialize_P(self):\n",
        "\n",
        "        self.P_output = self.add_weight(name='P_output', shape=(self.units, self.units), \n",
        "                                 initializer=keras.initializers.Identity(\n",
        "                                              gain=self.alpha_P), trainable=True)\n",
        "\n",
        "        if self.original_force_layer.recurrent_kernel.trainable:\n",
        "\n",
        "            identity_3d = np.zeros((self.units, self.units, self.units))\n",
        "            idx = np.arange(self.units)\n",
        "\n",
        "#################### \n",
        "\n",
        "            identity_3d[:, idx, idx] = self.alpha_P \n",
        "\n",
        "            if self.original_force_layer.recurrent_nontrainable_boolean_mask is not None:\n",
        "                I,J = np.nonzero(tf.transpose(self.original_force_layer.recurrent_nontrainable_boolean_mask).numpy()==True)\n",
        "                identity_3d[I,:,J]=0\n",
        "                identity_3d[I,J,:]=0\n",
        "\n",
        "#################### \n",
        "# # new \n",
        "#          #  print('new')\n",
        "#             identity_3d[idx, idx, :] = self.alpha_P \n",
        "#             J,I = np.nonzero(self.original_force_layer.recurrent_kernel.numpy()==0)\n",
        "#             identity_3d[J,:,I]=0\n",
        "#             identity_3d[:,J,I]=0\n",
        "\n",
        "#################### \n",
        "\n",
        "            self.P_GG = self.add_weight(name='P_GG', shape=(self.units, self.units, self.units), \n",
        "                                    initializer=keras.initializers.constant(identity_3d), \n",
        "                                    trainable=True)\n",
        "\n",
        "    def initialize_train_idx(self):\n",
        "        self._output_kernel_idx = None\n",
        "        self._recurrent_kernel_idx = None\n",
        "        for idx in range(len(self.trainable_variables)):\n",
        "          trainable_name = self.trainable_variables[idx].name\n",
        "              \n",
        "          if 'output_kernel' in trainable_name:\n",
        "            self._output_kernel_idx = idx\n",
        "          elif 'P_output' in trainable_name:\n",
        "            self._P_output_idx = idx\n",
        "          elif 'P_GG' in trainable_name:\n",
        "            self._P_GG_idx = idx\n",
        "          elif 'recurrent_kernel' in trainable_name:\n",
        "            self._recurrent_kernel_idx = idx\n",
        "\n",
        "    def call(self, x, training=False,   **kwargs):\n",
        "\n",
        "        if training:\n",
        "            return self.force_layer_call(x, training, **kwargs)\n",
        "        else:\n",
        "            initialization = all(v is None for v in self.force_layer.states)\n",
        "            \n",
        "            if not initialization:\n",
        "              original_state = [i.numpy() for i in self.force_layer.states]\n",
        "            output = self.force_layer_call(x, training, **kwargs)[0]\n",
        "\n",
        "            if not initialization:\n",
        "              self.force_layer.reset_states(states = original_state)\n",
        "            return output\n",
        "\n",
        "    def force_layer_call(self, x, training, **kwargs):\n",
        "        return self.force_layer(x, **kwargs) \n",
        "\n",
        "    def train_step(self, data):\n",
        "\n",
        "        x, y = data\n",
        "\n",
        "        if self.run_eagerly:\n",
        "          self.hidden_activation = []\n",
        "                \n",
        "\n",
        "        for i in range(x.shape[1]):\n",
        "          z, _, h, _ = self(x[:,i:i+1,:], training=True)\n",
        "\n",
        "          if self.force_layer.return_sequences:\n",
        "            z = z[:,0,:]\n",
        "         \n",
        "          trainable_vars = self.trainable_variables\n",
        "\n",
        "          if self._output_kernel_idx is not None:\n",
        "            self.update_output_kernel(self.P_output, h, z, y[:,i,:], \n",
        "                                      trainable_vars[self._P_output_idx], \n",
        "                                      trainable_vars[self._output_kernel_idx])\n",
        "          \n",
        "          if self._recurrent_kernel_idx is not None:\n",
        "            self.update_recurrent_kernel(self.P_GG, h, z, y[:,i,:],\n",
        "                                         trainable_vars[self._P_GG_idx],\n",
        "                                         trainable_vars[self._recurrent_kernel_idx])\n",
        "          \n",
        "        # Update metrics (includes the metric that tracks the loss)\n",
        "          self.compiled_metrics.update_state(y[:,i,:], z)\n",
        "        # Return a dict mapping metric names to current value\n",
        "\n",
        "          if self.run_eagerly:\n",
        "            self.hidden_activation.append(h.numpy()[0])\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def update_output_kernel(self, P_output, h, z, y, trainable_vars_P_output, trainable_vars_output_kernel):\n",
        "\n",
        "        # Compute pseudogradients\n",
        "        dP = self.pseudogradient_P(P_output, h)\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip([dP], [trainable_vars_P_output]))\n",
        "\n",
        "        dwO = self.pseudogradient_wO(P_output, h, z, y)\n",
        "        self.optimizer.apply_gradients(zip([dwO], [trainable_vars_output_kernel]))\n",
        "\n",
        "    def update_recurrent_kernel(self, P_Gx, h, z, y, trainable_vars_P_Gx, trainable_vars_recurrent_kernel):\n",
        "\n",
        "        # Compute pseudogradients\n",
        "        dP_Gx = self.pseudogradient_P_Gx(P_Gx, h)\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip([dP_Gx], [trainable_vars_P_Gx]))\n",
        "\n",
        "        dwR = self.pseudogradient_wR(P_Gx, h, z, y)\n",
        "        self.optimizer.apply_gradients(zip([dwR], [trainable_vars_recurrent_kernel]))\n",
        "\n",
        "\n",
        "    def pseudogradient_P(self, P, h):\n",
        "        # Implements the training step i.e. the rls() function\n",
        "        # This not a real gradient (does not use gradient.tape())\n",
        "        # Computes the actual update\n",
        "        # Example array shapes\n",
        "        # h : 1 x 500\n",
        "        # P : 500 x 500 \n",
        "        # k : 500 x 1 \n",
        "        # hPht : 1 x 1\n",
        "        # dP : 500 x 500 \n",
        "\n",
        "\n",
        "        k = backend.dot(P, tf.transpose(h))\n",
        "        hPht = backend.dot(h, k)\n",
        "        c = 1./(1.+hPht)\n",
        "      #  assert c.shape == (1,1)\n",
        "        #hP = backend.dot(h, P)\n",
        "        #dP = backend.dot(c*k, hP)\n",
        "        dP = backend.dot(c*k, tf.transpose(k))\n",
        "        return  dP \n",
        "\n",
        "    def pseudogradient_wO(self, P, h, z, y):\n",
        "        # z : 1 x 20 \n",
        "        # y : 1 x 20\n",
        "        # e : 1 x 20\n",
        "        # dwO : 500 x 20  \n",
        "\n",
        "        e = z-y\n",
        "        Ph = backend.dot(P, tf.transpose(h))\n",
        "        dwO = backend.dot(Ph, e)\n",
        "\n",
        "        return  dwO\n",
        "\n",
        "#################### \n",
        "\n",
        "    def pseudogradient_wR(self, P_Gx, h, z, y):\n",
        "        e = z - y \n",
        "        assert e.shape == (1,1), 'Output must only have 1 dimension'\n",
        "        Ph = backend.dot(P_Gx, tf.transpose(h))[:,:,0]\n",
        "\n",
        "        dwR = Ph*e ### only valid for 1-d output\n",
        "\n",
        "        return tf.transpose(dwR) \n",
        "\n",
        "    def pseudogradient_P_Gx(self, P_Gx, h):\n",
        "        Ph = backend.dot(P_Gx, tf.transpose(h))[:,:,0]\n",
        "        hPh = tf.expand_dims(backend.dot(Ph, tf.transpose(h)),axis = 2)\n",
        "        #htP = backend.dot(h, P_Gx)[0]\n",
        "        #dP_Gx = tf.expand_dims(Ph, axis = 2) * tf.expand_dims(htP, axis = 1)/(1+hPh)\n",
        "        dP_Gx = tf.expand_dims(Ph, axis = 2) * tf.expand_dims(Ph, axis = 1)/(1+hPh)\n",
        "        return dP_Gx\n",
        "\n",
        "#################### \n",
        "#new \n",
        "\n",
        "    # def pseudogradient_wR(self, P_Gx, h, z, y):\n",
        "    #     e = z - y \n",
        "    #     assert e.shape == (1,1)\n",
        "    #     Pht = backend.dot(h, P_Gx)[0] \n",
        "    #     dwR = e*Pht ### only valid for 1-d output\n",
        "\n",
        "    #     return dwR \n",
        "\n",
        "\n",
        "    # def pseudogradient_P_Gx(self, P_Gx, h):\n",
        "    #    Pht = backend.dot(h, P_Gx)      # get 1 by j by i\n",
        "    #    hPht = backend.dot(h, Pht)      # get 1 by 1 by i\n",
        "    #    hP = tf.tensordot(h, P_Gx, axes = [[1],[0]]) # get 1 by k by i\n",
        "    #    #dP_Gx = tf.reshape(Pht, (self.units, 1, self.units)) * hP / (1 + hPht)\n",
        "    #    dP_Gx = tf.expand_dims(Pht[0], axis = 1) * hP / (1 + hPht)\n",
        "\n",
        "    #    return dP_Gx\n",
        "\n",
        "#################### \n",
        "\n",
        "    def compile(self, metrics, **kwargs):\n",
        "        super().compile(optimizer=keras.optimizers.SGD(learning_rate=1), loss = 'mae', metrics=metrics,   **kwargs)\n",
        "\n",
        "\n",
        "    def fit(self, x, y=None, epochs = 1, verbose = 'auto', **kwargs):\n",
        "\n",
        "        if len(x.shape) < 2 or len(x.shape) > 3:\n",
        "            raise ValueError('Shape of x is invalid')\n",
        "\n",
        "        if len(y.shape) < 2 or len(y.shape) > 3:\n",
        "            raise ValueError('Shape of y is invalid')\n",
        "        \n",
        "        if len(x.shape) == 2:\n",
        "            x = tf.expand_dims(x, axis = 0)\n",
        "        \n",
        "        if len(y.shape) == 2:\n",
        "            y = tf.expand_dims(y, axis = 0)\n",
        "        \n",
        "        if x.shape[0] != 1:\n",
        "            raise ValueError(\"Dim 0 of x must be 1\")\n",
        "\n",
        "        if y.shape[0] != 1:\n",
        "            raise ValueError(\"Dim 0 of y must be 1\")\n",
        "        \n",
        "        if x.shape[1] != y.shape[1]: \n",
        "            raise ValueError('Timestep dimension of inputs must match')     \n",
        "\n",
        "        return super().fit(x = x, y = y, epochs = epochs, batch_size = 1, verbose = verbose, **kwargs)\n",
        "\n",
        "    def predict(self, x, **kwargs):\n",
        "        if len(x.shape) == 3 and x.shape[0] != 1:\n",
        "            raise ValueError('Dim 0 must be 1')\n",
        "        \n",
        "        if len(x.shape) < 2 or len(x.shape) > 3:\n",
        "            raise ValueError('')\n",
        "\n",
        "        if len(x.shape) == 2:\n",
        "            x = tf.expand_dims(x, axis = 0)\n",
        "\n",
        "        return self(x, training = False)[0]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRwezkJWMHnG"
      },
      "source": [
        "class EchoStateNetwork(FORCELayer):\n",
        "    def __init__(self, dtdivtau, hscale = 0.25, initial_a = None, **kwargs):\n",
        "        self.dtdivtau = dtdivtau \n",
        "        self.hscale = hscale\n",
        "        self._initial_a = initial_a\n",
        "        super().__init__(**kwargs)        \n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        \"\"\"Implements the forward step (i.e., the esn() function)\n",
        "        \"\"\"\n",
        "        prev_a, prev_h, prev_output = states      \n",
        "        input_term = backend.dot(inputs, self.input_kernel)\n",
        "        recurrent_term = backend.dot(prev_h, self.recurrent_kernel)\n",
        "        feedback_term = backend.dot(prev_output, self.feedback_kernel)\n",
        "\n",
        "        dadt = -prev_a + input_term + recurrent_term + feedback_term \n",
        "        a = prev_a + self.dtdivtau * dadt\n",
        "        h = self.activation(a)\n",
        "        output = backend.dot(h, self.output_kernel)\n",
        "\n",
        "        return output, [a, h, output]\n",
        "\n",
        "    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
        "\n",
        "        if self._initial_a is not None:\n",
        "          init_a = self._initial_a\n",
        "        else:\n",
        "          initializer = keras.initializers.RandomNormal(mean=0., \n",
        "                                                        stddev= self.hscale , \n",
        "                                                        seed = self.seed_gen.uniform([1], \n",
        "                                                                                        minval=None, \n",
        "                                                                                        dtype=tf.dtypes.int64)[0])\n",
        "          init_a = initializer((batch_size, self.units))  \n",
        "\n",
        "        init_h =  self.activation(init_a)\n",
        "        init_out = backend.dot(init_h,self.output_kernel) \n",
        "\n",
        "        return (init_a, init_h, init_out)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBCyF0PiMHuA"
      },
      "source": [
        "class NoFeedbackESN(EchoStateNetwork):\n",
        "\n",
        "    def __init__(self, recurrent_kernel_trainable  = True, **kwargs):\n",
        "        super().__init__(recurrent_kernel_trainable = recurrent_kernel_trainable, **kwargs)\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        \"\"\"Implements the forward step (i.e., the esn() function)\n",
        "        \"\"\"\n",
        "        prev_a, prev_h, prev_output = states      \n",
        "        input_term = backend.dot(inputs, self.input_kernel)\n",
        "        recurrent_term = backend.dot(prev_h, self.recurrent_kernel)\n",
        "\n",
        "        dadt = -prev_a + input_term + recurrent_term \n",
        "        a = prev_a + self.dtdivtau * dadt\n",
        "        h = self.activation(a)\n",
        "        output = backend.dot(h, self.output_kernel)\n",
        "\n",
        "        return output, [a, h, output]\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.initialize_input_kernel(input_shape[-1])\n",
        "        self.initialize_recurrent_kernel()\n",
        "        self.initialize_output_kernel()\n",
        "          \n",
        "        self.recurrent_nontrainable_boolean_mask = None\n",
        "        self.built = True\n",
        "\n",
        "    @classmethod\n",
        "    def from_weights(cls, weights, recurrent_nontrainable_boolean_mask, **kwargs):\n",
        "        # Initialize the network from a list of weights (e.g., user-generated)\n",
        "        input_kernel, recurrent_kernel, output_kernel = weights \n",
        "        input_shape, input_units = input_kernel.shape \n",
        "        recurrent_units1, recurrent_units2 = recurrent_kernel.shape \n",
        "        output_units, output_size = output_kernel.shape \n",
        "\n",
        "        units = input_units      \n",
        "\n",
        "        assert np.all(np.array([input_units, recurrent_units1, recurrent_units2, \n",
        "                            output_units]) == units)\n",
        "\n",
        "        assert 'p_recurr' not in kwargs.keys(), 'p_recurr not supported in this method'\n",
        "        assert recurrent_kernel.shape == recurrent_nontrainable_boolean_mask.shape, \"Boolean mask and recurrent kernel shape mis-match\"\n",
        "        assert tf.math.count_nonzero(tf.boolean_mask(recurrent_kernel, recurrent_nontrainable_boolean_mask)).numpy() == 0, \"Invalid boolean mask\"  \n",
        "\n",
        "        self = cls(units=units, output_size=output_size, p_recurr = None, **kwargs)\n",
        "\n",
        "        self.recurrent_nontrainable_boolean_mask = tf.convert_to_tensor(recurrent_nontrainable_boolean_mask)\n",
        "        \n",
        "        self.initialize_input_kernel(input_shape, input_kernel)\n",
        "        self.initialize_recurrent_kernel(recurrent_kernel)\n",
        "        self.initialize_output_kernel(output_kernel)\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "        return self"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_b77DtDWZlE"
      },
      "source": [
        "class secondFORCEModel(FORCEModel):\n",
        "\n",
        "    def initialize_P(self):\n",
        "\n",
        "        self.P_output = self.add_weight(name='P_output', shape=(self.units, self.units), \n",
        "                                        initializer=keras.initializers.Identity(gain=self.alpha_P), \n",
        "                                        trainable=True)\n",
        "\n",
        "        if self.original_force_layer.recurrent_kernel.trainable:\n",
        "\n",
        "            bool_mask = self.original_force_layer.recurrent_nontrainable_boolean_mask\n",
        "\n",
        "            if bool_mask is None or tf.math.count_nonzero(bool_mask) == 0:\n",
        "          #    print('bool mask None or count is 0')\n",
        "              self.P_GG = self.add_weight(name='P_GG', shape=(self.units, self.units), \n",
        "                                          initializer=keras.initializers.Identity(gain=self.alpha_P), \n",
        "                                          trainable=True)\n",
        "              \n",
        "              \n",
        "            else:\n",
        "              print('bool mask count is NOT zero')\n",
        "              identity_3d = np.zeros((self.units, self.units, self.units))\n",
        "              idx = np.arange(self.units)\n",
        "\n",
        "  #################### \n",
        "\n",
        "              identity_3d[:, idx, idx] = self.alpha_P \n",
        "\n",
        "              I,J = np.nonzero(tf.transpose(bool_mask).numpy()==True)\n",
        "              identity_3d[I,:,J]=0\n",
        "              identity_3d[I,J,:]=0\n",
        "\n",
        "  #################### \n",
        "  # # new \n",
        "  #          #  print('new')\n",
        "  #             identity_3d[idx, idx, :] = self.alpha_P \n",
        "  #             J,I = np.nonzero(self.original_force_layer.recurrent_kernel.numpy()==0)\n",
        "  #             identity_3d[J,:,I]=0\n",
        "  #             identity_3d[:,J,I]=0\n",
        "\n",
        "  #################### \n",
        "\n",
        "              self.P_GG = self.add_weight(name='P_GG', shape=(self.units, self.units, self.units), \n",
        "                                          initializer=keras.initializers.constant(identity_3d), \n",
        "                                          trainable=True)\n",
        "              \n",
        "    def pseudogradient_wR(self, P_Gx, h, z, y):\n",
        "        e = z - y \n",
        "        assert e.shape == (1,1), 'Output must only have 1 dimension'\n",
        "\n",
        "        if len(P_Gx.shape) == 2:\n",
        "           # print('wR len')\n",
        "            dwR_inter = backend.dot(P_Gx, tf.transpose(h))*e\n",
        "            return dwR_inter*tf.ones((P_Gx.shape))\n",
        "        else:\n",
        "            Ph = backend.dot(P_Gx, tf.transpose(h))[:,:,0]\n",
        "            dwR = Ph*e ### only valid for 1-d output\n",
        "            return tf.transpose(dwR) \n",
        "\n",
        "    def pseudogradient_P_Gx(self, P_Gx, h):\n",
        "\n",
        "        if len(P_Gx.shape) == 2:\n",
        "            #print('PGx len')\n",
        "            return self.pseudogradient_P(P_Gx,h)\n",
        "\n",
        "        Ph = backend.dot(P_Gx, tf.transpose(h))[:,:,0]\n",
        "        hPh = tf.expand_dims(backend.dot(Ph, tf.transpose(h)),axis = 2)\n",
        "        dP_Gx = tf.expand_dims(Ph, axis = 2) * tf.expand_dims(Ph, axis = 1)/(1+hPh)\n",
        "        return dP_Gx\n",
        "\n",
        "        "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmhznhB_1Xue",
        "outputId": "2f3e5cc6-0234-496b-e982-5781c9c92989"
      },
      "source": [
        "%%time \n",
        "start = time_time.time()\n",
        "\n",
        "wR = wR.numpy()\n",
        "np.fill_diagonal(wR, 0)\n",
        "wR = tf.convert_to_tensor(wR)\n",
        "bool_mask = wR == 0\n",
        "#bool_mask = tf.ones((n,n)) != 1\n",
        "\n",
        "print(tf.math.reduce_mean(tf.cast(bool_mask, tf.float32)))\n",
        "print()\n",
        "myesn2 = NoFeedbackESN.from_weights(weights = (wI, wR, wO), \n",
        "                                      dtdivtau=init_params['dt_over_tau'], \n",
        "                                      activation = 'tanh',\n",
        "                                       recurrent_nontrainable_boolean_mask = bool_mask, \n",
        "                                       initial_a = a0)\n",
        "\n",
        "model2 = FORCEModel(myesn2, return_sequences=True)  \n",
        "model2.compile(metrics=[\"mae\"] , run_eagerly = True)\n",
        "\n",
        "history2 = model2.fit(x=input2, y= f_t , epochs = ntraining)\n",
        "end = time_time.time()\n",
        "print('Seconds per epoch: ',f'{round(end-start,1)/ntraining}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0.0025, shape=(), dtype=float32)\n",
            "\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 77s 77s/step - mae: 0.0289\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 74s 74s/step - mae: 0.0304\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 75s 75s/step - mae: 0.0248\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 74s 74s/step - mae: 0.0264\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 74s 74s/step - mae: 0.0257\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 73s 73s/step - mae: 0.0209\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 75s 75s/step - mae: 0.0123\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 74s 74s/step - mae: 0.0181\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 74s 74s/step - mae: 0.0127\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 77s 77s/step - mae: 0.0128\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 72s 72s/step - mae: 0.0207\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 74s 74s/step - mae: 0.0222\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 72s 72s/step - mae: 0.0158\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 74s 74s/step - mae: 0.0137\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 73s 73s/step - mae: 0.0199\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 76s 76s/step - mae: 0.0224\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 74s 74s/step - mae: 0.0098\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 73s 73s/step - mae: 0.0100\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 75s 75s/step - mae: 0.0160\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 75s 75s/step - mae: 0.0150\n",
            "Seconds per epoch:  76.115\n",
            "CPU times: user 40min 2s, sys: 22.1 s, total: 40min 25s\n",
            "Wall time: 25min 22s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPCnxM2_9V3j",
        "outputId": "0d6aa316-6189-456a-babf-b02df91c9f74"
      },
      "source": [
        "np.diagonal(myesn2.recurrent_kernel.numpy())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXBqG1uW9i9M",
        "outputId": "8eda1d56-c14a-4c2b-c799-cb8b519a2ea2"
      },
      "source": [
        "np.diagonal(wR.numpy())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LOAJ05fdpXp",
        "outputId": "73ab3a57-47ee-493f-de22-e5e387528d64"
      },
      "source": [
        "%%time\n",
        "start = time_time.time()\n",
        "bool_mask = tf.ones((n,n)) != 1\n",
        "myesn2 = NoFeedbackESN.from_weights(weights = (wI, wR, wO), \n",
        "                                      dtdivtau=init_params['dt_over_tau'], \n",
        "                                      activation = 'tanh',\n",
        "                                       recurrent_nontrainable_boolean_mask = bool_mask, \n",
        "                                       initial_a = a0)\n",
        "\n",
        "model2 = FORCEModel(myesn2, return_sequences=True)  \n",
        "model2.compile(metrics=[\"mae\"] , run_eagerly = True)\n",
        "\n",
        "history2 = model2.fit(x=input2, y= f_t , epochs = ntraining)\n",
        "end = time_time.time()\n",
        "print('Seconds per epoch: ',f'{round(end-start,1)/ntraining}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1/1 [==============================] - 71s 71s/step - mae: 0.0289\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 71s 71s/step - mae: 0.0300\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 72s 72s/step - mae: 0.0234\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 72s 72s/step - mae: 0.0265\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 71s 71s/step - mae: 0.0262\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 71s 71s/step - mae: 0.0193\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 71s 71s/step - mae: 0.0183\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 71s 71s/step - mae: 0.0271\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 72s 72s/step - mae: 0.0183\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 69s 69s/step - mae: 0.0157\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 72s 72s/step - mae: 0.0216\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 70s 70s/step - mae: 0.0184\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 73s 73s/step - mae: 0.0153\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 69s 69s/step - mae: 0.0116\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 71s 71s/step - mae: 0.0138\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 71s 71s/step - mae: 0.0175\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 74s 74s/step - mae: 0.0077\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 70s 70s/step - mae: 0.0085\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 70s 70s/step - mae: 0.0122\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 69s 69s/step - mae: 0.0111\n",
            "Seconds per epoch:  71.02000000000001\n",
            "CPU times: user 38min 12s, sys: 8.72 s, total: 38min 21s\n",
            "Wall time: 23min 40s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5MqrdfMdpg-",
        "outputId": "1e07629f-82e8-4d96-cdb3-993e4ecf8856"
      },
      "source": [
        "np.diagonal(myesn2.recurrent_kernel.numpy())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.01170432,  0.03379069, -0.03195122,  0.0445899 ,  0.0501068 ,\n",
              "       -0.02229326,  0.00382762,  0.01983151,  0.12214473,  0.00242615,\n",
              "        0.05157316, -0.02733548, -0.02111337,  0.04538266,  0.00200173,\n",
              "       -0.02401695, -0.05029807, -0.05429257,  0.02734522, -0.01736236,\n",
              "        0.01622583, -0.07090793,  0.05781898,  0.00932593, -0.03284219,\n",
              "        0.07053553, -0.03637595, -0.02905682,  0.09198544,  0.00527502,\n",
              "        0.03581332,  0.01031048, -0.08506119,  0.01068352,  0.02045452,\n",
              "       -0.00258599,  0.01506071,  0.03377052,  0.01563763, -0.01687369,\n",
              "       -0.000776  , -0.0128849 , -0.0094495 ,  0.07666625, -0.00844504,\n",
              "        0.0603042 , -0.03435494, -0.09966249,  0.04538142,  0.02583141,\n",
              "       -0.02017155, -0.05427522,  0.0235228 , -0.04752948,  0.00351077,\n",
              "        0.02968894,  0.00098787, -0.07781325,  0.05562407, -0.08233822,\n",
              "        0.03214607, -0.00844648,  0.06274369, -0.02784043,  0.05954359,\n",
              "       -0.00526223,  0.07202754,  0.01271846,  0.01028233, -0.04437561,\n",
              "        0.0396862 ,  0.0457352 ,  0.00236366,  0.09173545,  0.05143734,\n",
              "       -0.02140702,  0.0057998 , -0.02779897,  0.05140074,  0.02107896,\n",
              "        0.05208615, -0.04049086,  0.04098546,  0.07669767,  0.00927914,\n",
              "       -0.00776965, -0.01876302,  0.02464629,  0.01827843,  0.01537888,\n",
              "        0.06565873, -0.00166598,  0.0189442 , -0.01821343, -0.00323541,\n",
              "        0.02193921, -0.0504669 ,  0.04277806,  0.02809405, -0.03819122,\n",
              "        0.02399792,  0.01957835,  0.05415459, -0.01298102, -0.06803946,\n",
              "        0.06165672,  0.08186038, -0.00280465,  0.03646499, -0.04582623,\n",
              "        0.0505731 , -0.00928458, -0.03213998,  0.04507109,  0.03428781,\n",
              "       -0.02633919,  0.00250674, -0.00253434,  0.01773972, -0.05903597,\n",
              "        0.02421142, -0.01013671,  0.03477813,  0.03287545,  0.02530972,\n",
              "       -0.03311767, -0.00685485, -0.05142731, -0.10800808, -0.03056582,\n",
              "        0.00407188,  0.06616654,  0.01800174,  0.00867081,  0.04660049,\n",
              "       -0.00223451, -0.08022758,  0.00666961,  0.09027763, -0.0018466 ,\n",
              "       -0.02141471, -0.02046209,  0.02893589, -0.02214944, -0.05990913,\n",
              "        0.05111954,  0.04322224,  0.00685291,  0.03888924, -0.00834955,\n",
              "        0.07933192,  0.03074444,  0.02146565, -0.04985551,  0.02719523,\n",
              "       -0.09309515,  0.08884037,  0.12055447, -0.0362297 ,  0.06412764,\n",
              "       -0.00529417, -0.04058234,  0.02125491, -0.02010703,  0.08127361,\n",
              "       -0.03793859, -0.02608549,  0.02930399, -0.01957797, -0.02117961,\n",
              "        0.00617031, -0.0016699 ,  0.04786423,  0.04147431,  0.06083793,\n",
              "       -0.03736563,  0.00847371,  0.05501799,  0.06030799,  0.012686  ,\n",
              "        0.00610886,  0.01845117,  0.07365361,  0.07211578, -0.00178494,\n",
              "       -0.03241962,  0.01660456,  0.05622744, -0.01280123, -0.01929319,\n",
              "       -0.03497361, -0.03136525,  0.04006482, -0.02037276,  0.07605318,\n",
              "       -0.03264651,  0.013573  ,  0.00446503, -0.04067702, -0.06057796,\n",
              "       -0.0188443 ,  0.00232285,  0.00062146, -0.01685965,  0.03555596,\n",
              "       -0.0094142 , -0.07551682, -0.03853585, -0.02729107, -0.01688749,\n",
              "       -0.02038923,  0.00362908,  0.00574955, -0.00826515, -0.03570519,\n",
              "       -0.0678874 , -0.00636562, -0.01165794, -0.06118509,  0.04133226,\n",
              "       -0.03818515,  0.05176659,  0.03581411, -0.05641232,  0.02641049,\n",
              "        0.00748978,  0.00883994, -0.08760011, -0.0214137 ,  0.00056629,\n",
              "       -0.01675082, -0.00242562, -0.00997463,  0.00622061, -0.02020735,\n",
              "       -0.00727714,  0.03961161, -0.0059955 , -0.03119875,  0.02413108,\n",
              "        0.02340614, -0.06102505, -0.00402324, -0.00213142, -0.02261598,\n",
              "       -0.02389375,  0.01930627,  0.03786568, -0.00933348,  0.02937752,\n",
              "        0.03086439,  0.05162126,  0.00393636,  0.00607101,  0.00530316,\n",
              "       -0.02070925, -0.02419987, -0.03113391,  0.00761151,  0.02129736,\n",
              "        0.04738355,  0.02070305, -0.00261594,  0.03376916,  0.01915069,\n",
              "        0.0133073 , -0.02621488,  0.01033133, -0.054345  ,  0.09860291,\n",
              "        0.01856586, -0.04095653, -0.02406994,  0.0097643 , -0.01423544,\n",
              "        0.00067405, -0.04205293,  0.01403115,  0.01757014,  0.0436644 ,\n",
              "        0.02079397, -0.03000027,  0.00276045,  0.03552273, -0.05312159,\n",
              "        0.00575147, -0.03935692,  0.03230222, -0.07869091,  0.02305229,\n",
              "       -0.0314826 ,  0.05461077,  0.02339516, -0.0374936 , -0.02570859,\n",
              "        0.0769376 , -0.00818261, -0.01564664, -0.02365452,  0.01897473,\n",
              "       -0.01432062, -0.00516232,  0.00526434,  0.09640071,  0.07483678,\n",
              "        0.01146836, -0.06199032,  0.05957411, -0.01980992,  0.05161945,\n",
              "       -0.0358142 ,  0.05791093,  0.04679305,  0.02451609,  0.05205695,\n",
              "        0.11635894,  0.00237413,  0.03279774,  0.01698468, -0.04925212,\n",
              "        0.09102729,  0.05469593, -0.01254677,  0.10178501,  0.01185682,\n",
              "       -0.05019436, -0.08108631, -0.00954711,  0.01727378, -0.0407499 ,\n",
              "       -0.00583033,  0.00035994, -0.01766652, -0.01372758,  0.00643422,\n",
              "        0.03450306,  0.01165587, -0.02650791,  0.04116676, -0.00841267,\n",
              "        0.03672994, -0.02244523,  0.11564939, -0.02382809, -0.02188341,\n",
              "       -0.04405818,  0.12003464,  0.00637131,  0.05185204, -0.07868088,\n",
              "       -0.00568126,  0.01029586,  0.05820096,  0.00586533, -0.03316946,\n",
              "        0.06054908, -0.01991651,  0.02638472,  0.01220607,  0.08077812,\n",
              "        0.05402083,  0.03274433, -0.02382634, -0.01286094, -0.09240939,\n",
              "        0.06864004,  0.04839225, -0.0522496 ,  0.03395623, -0.02524871,\n",
              "        0.1058271 ,  0.00825974,  0.02427313, -0.04765395,  0.06416615,\n",
              "        0.06588905,  0.04079712,  0.00749489,  0.02900236,  0.07803022,\n",
              "        0.05210532,  0.09539602,  0.05564428, -0.1018154 ,  0.03750237,\n",
              "       -0.07982231,  0.01821428, -0.01228832,  0.05292891,  0.03728349,\n",
              "       -0.00312437,  0.02295749, -0.00392615, -0.03824162, -0.12605919,\n",
              "        0.03169473,  0.08322471, -0.02716788, -0.00681119, -0.06128756],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlOjTDeNrNr6"
      },
      "source": [
        "# fig, (ax1, ax2) = plt.subplots(1, 2, sharey = True, figsize=(24,12))\n",
        "\n",
        "\n",
        "# ax1.plot(time , f_t  + 2*np.arange(0, f_t.shape[1]), 'g')\n",
        "# ax1.plot(time , z_t_call_2 + 2*np.arange(0, z_t_call_2.shape[1]), 'r');\n",
        " \n",
        "# ax1.set_xlim((0, T))\n",
        "# plt.title('Target - f (green), Output - z (red)')\n",
        "# ax1.set_xlabel('Time')\n",
        "# ax1.set_ylabel('Dimension')\n",
        "\n",
        "# ax2.plot(time, tf.math.abs(f_t-z_t_call_2) + 2*np.arange(0, z_t_call_2.shape[1]), 'r');\n",
        "# ax2.set_xlim((0, T))\n",
        "# plt.title('MAE')\n",
        "# ax2.set_xlabel('Time')\n",
        "# ax2.set_ylabel('Dimension')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgFVUemGJEoC"
      },
      "source": [
        "# plt.figure(figsize=(12,8), dpi=80)\n",
        "# plt.plot(history2.history['mae'])"
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}
