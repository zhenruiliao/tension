{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Issue - P_recurrent identical for every slice.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qlUKH0DfAzK",
        "outputId": "e3b2173e-5b23-4c82-8956-6d2629c3e0ec"
      },
      "source": [
        "!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda$(echo $CUDA_VERSION | sed -e 's/\\.//' -e 's/\\..*//')/jaxlib-0.1.8-cp36-none-linux_x86_64.whl\n",
        "!pip install --upgrade -q jax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: jaxlib-0.1.8-cp36-none-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 745 kB 5.1 MB/s \n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO5p-pUN7Mhh"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import backend, activations"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G7jAkKMMHgF"
      },
      "source": [
        "class FORCELayer(keras.layers.AbstractRNNCell):\n",
        "    def __init__(self, units, output_size, activation, seed = None, g = 1.5, \n",
        "                 input_kernel_trainable = False, recurrent_kernel_trainable = False, \n",
        "                 output_kernel_trainable = True, feedback_kernel_trainable = False, p_recurr = 1, **kwargs):\n",
        "                \n",
        "        self.units = units \n",
        "        self._output_size = output_size\n",
        "        self.activation = activations.get(activation)\n",
        "\n",
        "        if seed is None:\n",
        "          self.seed_gen = tf.random.Generator.from_non_deterministic_state()\n",
        "        else:\n",
        "          self.seed_gen = tf.random.Generator.from_seed(seed)\n",
        "        \n",
        "        self._g = g\n",
        "\n",
        "        self._input_kernel_trainable = input_kernel_trainable\n",
        "        self._recurrent_kernel_trainable = recurrent_kernel_trainable\n",
        "        self._feedback_kernel_trainable = feedback_kernel_trainable\n",
        "        self._output_kernel_trainable = output_kernel_trainable\n",
        "        self._p_recurr = p_recurr\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    @property\n",
        "    def state_size(self):\n",
        "        return [self.units, self.units, self.output_size]\n",
        "\n",
        "    @property \n",
        "    def output_size(self):\n",
        "        return self._output_size\n",
        "\n",
        "    def initialize_input_kernel(self, input_shape, input_kernel = None):\n",
        "        if input_kernel is None:\n",
        "            initializer = keras.initializers.RandomNormal(mean=0., \n",
        "                                                          stddev= 1/input_shape**0.5, \n",
        "                                                          seed=self.seed_gen.uniform([1], \n",
        "                                                                                    minval=None, \n",
        "                                                                                    dtype=tf.dtypes.int64)[0])\n",
        "            input_kernel = initializer(shape = (input_shape, self.units))\n",
        "         \n",
        "        self.input_kernel = self.add_weight(shape=(input_shape, self.units),\n",
        "                                            initializer=keras.initializers.constant(input_kernel),\n",
        "                                            trainable = self._input_kernel_trainable,\n",
        "                                            name='input_kernel')\n",
        "        \n",
        "    def initialize_recurrent_kernel(self, recurrent_kernel = None):\n",
        "        if recurrent_kernel is None:        \n",
        "            initializer = keras.initializers.RandomNormal(mean=0., \n",
        "                                                          stddev= self._g/self.units**0.5, \n",
        "                                                          seed=self.seed_gen.uniform([1], \n",
        "                                                                                      minval=None, \n",
        "                                                                                      dtype=tf.dtypes.int64)[0])\n",
        "        \n",
        "            recurrent_kernel = self._p_recurr*keras.layers.Dropout(1-self._p_recurr)(initializer(shape = (self.units, self.units)), \n",
        "                                                                                    training = True)\n",
        "\n",
        "        self.recurrent_kernel = self.add_weight(shape=(self.units, self.units),\n",
        "                                                initializer=keras.initializers.constant(recurrent_kernel),\n",
        "                                                trainable = self._recurrent_kernel_trainable,\n",
        "                                                name='recurrent_kernel')\n",
        "    \n",
        "    def initialize_feedback_kernel(self, feedback_kernel = None):\n",
        "        if feedback_kernel is None:\n",
        "            initializer = keras.initializers.RandomNormal(mean=0., \n",
        "                                                          stddev= 1, \n",
        "                                                          seed=self.seed_gen.uniform([1], \n",
        "                                                                                 minval=None, \n",
        "                                                                                 dtype=tf.dtypes.int64)[0])\n",
        "            feedback_kernel = initializer(shape = (self.output_size, self.units))\n",
        "\n",
        "        self.feedback_kernel = self.add_weight(shape=(self.output_size, self.units),\n",
        "                                                initializer=keras.initializers.constant(feedback_kernel),\n",
        "                                                trainable = self._feedback_kernel_trainable,\n",
        "                                                name='feedback_kernel')\n",
        "                                            \n",
        "\n",
        "    def initialize_output_kernel(self, output_kernel = None):\n",
        "        if output_kernel is None:\n",
        "            initializer=keras.initializers.RandomNormal(mean=0., \n",
        "                                                        stddev= 1/self.units**0.5, \n",
        "                                                        seed=self.seed_gen.uniform([1], \n",
        "                                                                                   minval=None, \n",
        "                                                                                   dtype=tf.dtypes.int64)[0])\n",
        "            output_kernel = initializer(shape = (self.units, self.output_size))\n",
        "\n",
        "        self.output_kernel = self.add_weight(shape=(self.units, self.output_size),\n",
        "                                              initializer=keras.initializers.constant(output_kernel),\n",
        "                                              trainable = self._output_kernel_trainable,\n",
        "                                              name='output_kernel')     \n",
        "    \n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.initialize_input_kernel(input_shape[-1])\n",
        "        self.initialize_recurrent_kernel()\n",
        "        self.initialize_feedback_kernel()\n",
        "        self.initialize_output_kernel() \n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    @classmethod\n",
        "    def from_weights(cls, weights, **kwargs):\n",
        "        # Initialize the network from a list of weights (e.g., user-generated)\n",
        "        input_kernel, recurrent_kernel, feedback_kernel, output_kernel = weights \n",
        "        input_shape, input_units = input_kernel.shape \n",
        "        recurrent_units1, recurrent_units2 = recurrent_kernel.shape \n",
        "        feedback_output_size, feedback_units = feedback_kernel.shape \n",
        "        output_units, output_size = output_kernel.shape \n",
        "\n",
        "\n",
        "        units = input_units \n",
        "        assert np.all(np.array([input_units, recurrent_units1, recurrent_units2, \n",
        "                            feedback_units, output_units]) == units)\n",
        "\n",
        "        assert feedback_output_size == output_size \n",
        "        assert 'p_recurr' not in kwargs.keys(), 'p_recurr not supported in this method'\n",
        "\n",
        "        self = cls(units=units, output_size=output_size, p_recurr = None, **kwargs)\n",
        "\n",
        "        self.initialize_input_kernel(input_shape, input_kernel)\n",
        "        self.initialize_recurrent_kernel(recurrent_kernel)\n",
        "        self.initialize_feedback_kernel(feedback_kernel)\n",
        "        self.initialize_output_kernel(output_kernel)\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "        return self\n",
        "\n",
        "class FORCEModel(keras.Model):\n",
        "    def __init__(self, force_layer, alpha_P=1.,return_sequences=True):\n",
        "        super().__init__()\n",
        "        self.alpha_P = alpha_P\n",
        "        self.force_layer = keras.layers.RNN(force_layer, \n",
        "                                            stateful=True, \n",
        "                                            return_state=True, \n",
        "                                            return_sequences=return_sequences)\n",
        "\n",
        "        self.units = force_layer.units \n",
        "\n",
        "        self.original_force_layer = force_layer\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super().build(input_shape)\n",
        "        self.initialize_P()\n",
        "        self.initialize_train_idx()\n",
        "\n",
        "    def initialize_P(self):\n",
        "\n",
        "        self.P_output = self.add_weight(name='P_output', shape=(self.units, self.units), \n",
        "                                 initializer=keras.initializers.Identity(\n",
        "                                              gain=self.alpha_P), trainable=True)\n",
        "\n",
        "        if self.original_force_layer.recurrent_kernel.trainable:\n",
        "\n",
        "            identity_3d = np.zeros((self.units, self.units, self.units))\n",
        "            idx = np.arange(self.units)\n",
        "\n",
        "#################### \n",
        "\n",
        "            identity_3d[:, idx, idx] = self.alpha_P \n",
        "\n",
        "            if self.original_force_layer.recurrent_nontrainable_boolean_mask is not None:\n",
        "                I,J = np.nonzero(tf.transpose(self.original_force_layer.recurrent_nontrainable_boolean_mask).numpy()==True)\n",
        "                identity_3d[I,:,J]=0\n",
        "                identity_3d[I,J,:]=0\n",
        "\n",
        "#################### \n",
        "# # new \n",
        "#          #  print('new')\n",
        "#             identity_3d[idx, idx, :] = self.alpha_P \n",
        "#             J,I = np.nonzero(self.original_force_layer.recurrent_kernel.numpy()==0)\n",
        "#             identity_3d[J,:,I]=0\n",
        "#             identity_3d[:,J,I]=0\n",
        "\n",
        "#################### \n",
        "\n",
        "            self.P_GG = self.add_weight(name='P_GG', shape=(self.units, self.units, self.units), \n",
        "                                    initializer=keras.initializers.constant(identity_3d), \n",
        "                                    trainable=True)\n",
        "\n",
        "    def initialize_train_idx(self):\n",
        "        self._output_kernel_idx = None\n",
        "        self._recurrent_kernel_idx = None\n",
        "        for idx in range(len(self.trainable_variables)):\n",
        "          trainable_name = self.trainable_variables[idx].name\n",
        "              \n",
        "          if 'output_kernel' in trainable_name:\n",
        "            self._output_kernel_idx = idx\n",
        "          elif 'P_output' in trainable_name:\n",
        "            self._P_output_idx = idx\n",
        "          elif 'P_GG' in trainable_name:\n",
        "            self._P_GG_idx = idx\n",
        "          elif 'recurrent_kernel' in trainable_name:\n",
        "            self._recurrent_kernel_idx = idx\n",
        "\n",
        "    def call(self, x, training=False,   **kwargs):\n",
        "\n",
        "        if training:\n",
        "            return self.force_layer_call(x, training, **kwargs)\n",
        "        else:\n",
        "            initialization = all(v is None for v in self.force_layer.states)\n",
        "            \n",
        "            if not initialization:\n",
        "              original_state = [i.numpy() for i in self.force_layer.states]\n",
        "            output = self.force_layer_call(x, training, **kwargs)[0]\n",
        "\n",
        "            if not initialization:\n",
        "              self.force_layer.reset_states(states = original_state)\n",
        "            return output\n",
        "\n",
        "    def force_layer_call(self, x, training, **kwargs):\n",
        "        return self.force_layer(x, **kwargs) \n",
        "\n",
        "    def train_step(self, data):\n",
        "\n",
        "        x, y = data\n",
        "\n",
        "        if self.run_eagerly:\n",
        "          self.hidden_activation = []\n",
        "                \n",
        "\n",
        "        for i in range(x.shape[1]):\n",
        "          z, _, h, _ = self(x[:,i:i+1,:], training=True)\n",
        "\n",
        "          if self.force_layer.return_sequences:\n",
        "            z = z[:,0,:]\n",
        "         \n",
        "          trainable_vars = self.trainable_variables\n",
        "\n",
        "          if self._output_kernel_idx is not None:\n",
        "            self.update_output_kernel(self.P_output, h, z, y[:,i,:], \n",
        "                                      trainable_vars[self._P_output_idx], \n",
        "                                      trainable_vars[self._output_kernel_idx])\n",
        "          \n",
        "          if self._recurrent_kernel_idx is not None:\n",
        "            self.update_recurrent_kernel(self.P_GG, h, z, y[:,i,:],\n",
        "                                         trainable_vars[self._P_GG_idx],\n",
        "                                         trainable_vars[self._recurrent_kernel_idx])\n",
        "          \n",
        "        # Update metrics (includes the metric that tracks the loss)\n",
        "          self.compiled_metrics.update_state(y[:,i,:], z)\n",
        "        # Return a dict mapping metric names to current value\n",
        "\n",
        "          if self.run_eagerly:\n",
        "            self.hidden_activation.append(h.numpy()[0])\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def update_output_kernel(self, P_output, h, z, y, trainable_vars_P_output, trainable_vars_output_kernel):\n",
        "\n",
        "        # Compute pseudogradients\n",
        "        dP = self.pseudogradient_P(P_output, h)\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip([dP], [trainable_vars_P_output]))\n",
        "\n",
        "        dwO = self.pseudogradient_wO(P_output, h, z, y)\n",
        "        self.optimizer.apply_gradients(zip([dwO], [trainable_vars_output_kernel]))\n",
        "\n",
        "    def update_recurrent_kernel(self, P_Gx, h, z, y, trainable_vars_P_Gx, trainable_vars_recurrent_kernel):\n",
        "\n",
        "        # Compute pseudogradients\n",
        "        dP_Gx = self.pseudogradient_P_Gx(P_Gx, h)\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip([dP_Gx], [trainable_vars_P_Gx]))\n",
        "\n",
        "        dwR = self.pseudogradient_wR(P_Gx, h, z, y)\n",
        "        self.optimizer.apply_gradients(zip([dwR], [trainable_vars_recurrent_kernel]))\n",
        "\n",
        "\n",
        "    def pseudogradient_P(self, P, h):\n",
        "        # Implements the training step i.e. the rls() function\n",
        "        # This not a real gradient (does not use gradient.tape())\n",
        "        # Computes the actual update\n",
        "        # Example array shapes\n",
        "        # h : 1 x 500\n",
        "        # P : 500 x 500 \n",
        "        # k : 500 x 1 \n",
        "        # hPht : 1 x 1\n",
        "        # dP : 500 x 500 \n",
        "\n",
        "\n",
        "        k = backend.dot(P, tf.transpose(h))\n",
        "        hPht = backend.dot(h, k)\n",
        "        c = 1./(1.+hPht)\n",
        "      #  assert c.shape == (1,1)\n",
        "        #hP = backend.dot(h, P)\n",
        "        #dP = backend.dot(c*k, hP)\n",
        "        dP = backend.dot(c*k, tf.transpose(k))\n",
        "        return  dP \n",
        "\n",
        "    def pseudogradient_wO(self, P, h, z, y):\n",
        "        # z : 1 x 20 \n",
        "        # y : 1 x 20\n",
        "        # e : 1 x 20\n",
        "        # dwO : 500 x 20  \n",
        "\n",
        "        e = z-y\n",
        "        Ph = backend.dot(P, tf.transpose(h))\n",
        "        dwO = backend.dot(Ph, e)\n",
        "\n",
        "        return  dwO\n",
        "\n",
        "#################### \n",
        "\n",
        "    def pseudogradient_wR(self, P_Gx, h, z, y):\n",
        "        e = z - y \n",
        "        assert e.shape == (1,1), 'Output must only have 1 dimension'\n",
        "        Ph = backend.dot(P_Gx, tf.transpose(h))[:,:,0]\n",
        "\n",
        "        dwR = Ph*e ### only valid for 1-d output\n",
        "\n",
        "        return tf.transpose(dwR) \n",
        "\n",
        "    def pseudogradient_P_Gx(self, P_Gx, h):\n",
        "        Ph = backend.dot(P_Gx, tf.transpose(h))[:,:,0]\n",
        "        hPh = tf.expand_dims(backend.dot(Ph, tf.transpose(h)),axis = 2)\n",
        "        #htP = backend.dot(h, P_Gx)[0]\n",
        "        #dP_Gx = tf.expand_dims(Ph, axis = 2) * tf.expand_dims(htP, axis = 1)/(1+hPh)\n",
        "        dP_Gx = tf.expand_dims(Ph, axis = 2) * tf.expand_dims(Ph, axis = 1)/(1+hPh)\n",
        "        return dP_Gx\n",
        "\n",
        "#################### \n",
        "#new \n",
        "\n",
        "    # def pseudogradient_wR(self, P_Gx, h, z, y):\n",
        "    #     e = z - y \n",
        "    #     assert e.shape == (1,1)\n",
        "    #     Pht = backend.dot(h, P_Gx)[0] \n",
        "    #     dwR = e*Pht ### only valid for 1-d output\n",
        "\n",
        "    #     return dwR \n",
        "\n",
        "\n",
        "    # def pseudogradient_P_Gx(self, P_Gx, h):\n",
        "    #    Pht = backend.dot(h, P_Gx)      # get 1 by j by i\n",
        "    #    hPht = backend.dot(h, Pht)      # get 1 by 1 by i\n",
        "    #    hP = tf.tensordot(h, P_Gx, axes = [[1],[0]]) # get 1 by k by i\n",
        "    #    #dP_Gx = tf.reshape(Pht, (self.units, 1, self.units)) * hP / (1 + hPht)\n",
        "    #    dP_Gx = tf.expand_dims(Pht[0], axis = 1) * hP / (1 + hPht)\n",
        "\n",
        "    #    return dP_Gx\n",
        "\n",
        "#################### \n",
        "\n",
        "    def compile(self, metrics, **kwargs):\n",
        "        super().compile(optimizer=keras.optimizers.SGD(learning_rate=1), loss = 'mae', metrics=metrics,   **kwargs)\n",
        "\n",
        "\n",
        "    def fit(self, x, y=None, epochs = 1, verbose = 'auto', **kwargs):\n",
        "\n",
        "        if len(x.shape) < 2 or len(x.shape) > 3:\n",
        "            raise ValueError('Shape of x is invalid')\n",
        "\n",
        "        if len(y.shape) < 2 or len(y.shape) > 3:\n",
        "            raise ValueError('Shape of y is invalid')\n",
        "        \n",
        "        if len(x.shape) == 2:\n",
        "            x = tf.expand_dims(x, axis = 0)\n",
        "        \n",
        "        if len(y.shape) == 2:\n",
        "            y = tf.expand_dims(y, axis = 0)\n",
        "        \n",
        "        if x.shape[0] != 1:\n",
        "            raise ValueError(\"Dim 0 of x must be 1\")\n",
        "\n",
        "        if y.shape[0] != 1:\n",
        "            raise ValueError(\"Dim 0 of y must be 1\")\n",
        "        \n",
        "        if x.shape[1] != y.shape[1]: \n",
        "            raise ValueError('Timestep dimension of inputs must match')     \n",
        "\n",
        "        return super().fit(x = x, y = y, epochs = epochs, batch_size = 1, verbose = verbose, **kwargs)\n",
        "\n",
        "    def predict(self, x, **kwargs):\n",
        "        if len(x.shape) == 3 and x.shape[0] != 1:\n",
        "            raise ValueError('Dim 0 must be 1')\n",
        "        \n",
        "        if len(x.shape) < 2 or len(x.shape) > 3:\n",
        "            raise ValueError('')\n",
        "\n",
        "        if len(x.shape) == 2:\n",
        "            x = tf.expand_dims(x, axis = 0)\n",
        "\n",
        "        return self(x, training = False)[0]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBCyF0PiMHuA"
      },
      "source": [
        "class EchoStateNetwork(FORCELayer):\n",
        "    def __init__(self, dtdivtau, hscale = 0.25, initial_a = None, **kwargs):\n",
        "        self.dtdivtau = dtdivtau \n",
        "        self.hscale = hscale\n",
        "        self._initial_a = initial_a\n",
        "        super().__init__(**kwargs)        \n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        \"\"\"Implements the forward step (i.e., the esn() function)\n",
        "        \"\"\"\n",
        "        prev_a, prev_h, prev_output = states      \n",
        "        input_term = backend.dot(inputs, self.input_kernel)\n",
        "        recurrent_term = backend.dot(prev_h, self.recurrent_kernel)\n",
        "        feedback_term = backend.dot(prev_output, self.feedback_kernel)\n",
        "\n",
        "        dadt = -prev_a + input_term + recurrent_term + feedback_term \n",
        "        a = prev_a + self.dtdivtau * dadt\n",
        "        h = self.activation(a)\n",
        "        output = backend.dot(h, self.output_kernel)\n",
        "\n",
        "        return output, [a, h, output]\n",
        "\n",
        "    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
        "\n",
        "        if self._initial_a is not None:\n",
        "          init_a = self._initial_a\n",
        "        else:\n",
        "          initializer = keras.initializers.RandomNormal(mean=0., \n",
        "                                                        stddev= self.hscale , \n",
        "                                                        seed = self.seed_gen.uniform([1], \n",
        "                                                                                        minval=None, \n",
        "                                                                                        dtype=tf.dtypes.int64)[0])\n",
        "          init_a = initializer((batch_size, self.units))  \n",
        "\n",
        "        init_h =  self.activation(init_a)\n",
        "        init_out = backend.dot(init_h,self.output_kernel) \n",
        "\n",
        "        return (init_a, init_h, init_out)\n",
        "\n",
        "class NoFeedbackESN(EchoStateNetwork):\n",
        "\n",
        "    def __init__(self, recurrent_kernel_trainable  = True, **kwargs):\n",
        "        super().__init__(recurrent_kernel_trainable = recurrent_kernel_trainable, **kwargs)\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        \"\"\"Implements the forward step (i.e., the esn() function)\n",
        "        \"\"\"\n",
        "        prev_a, prev_h, prev_output = states      \n",
        "        input_term = backend.dot(inputs, self.input_kernel)\n",
        "        recurrent_term = backend.dot(prev_h, self.recurrent_kernel)\n",
        "\n",
        "        dadt = -prev_a + input_term + recurrent_term \n",
        "        a = prev_a + self.dtdivtau * dadt\n",
        "        h = self.activation(a)\n",
        "        output = backend.dot(h, self.output_kernel)\n",
        "\n",
        "        return output, [a, h, output]\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.initialize_input_kernel(input_shape[-1])\n",
        "        self.initialize_recurrent_kernel()\n",
        "        self.initialize_output_kernel()\n",
        "          \n",
        "        self.recurrent_nontrainable_boolean_mask = None\n",
        "        self.built = True\n",
        "\n",
        "    @classmethod\n",
        "    def from_weights(cls, weights, recurrent_nontrainable_boolean_mask, **kwargs):\n",
        "        # Initialize the network from a list of weights (e.g., user-generated)\n",
        "        input_kernel, recurrent_kernel, output_kernel = weights \n",
        "        input_shape, input_units = input_kernel.shape \n",
        "        recurrent_units1, recurrent_units2 = recurrent_kernel.shape \n",
        "        output_units, output_size = output_kernel.shape \n",
        "\n",
        "        units = input_units      \n",
        "\n",
        "        assert np.all(np.array([input_units, recurrent_units1, recurrent_units2, \n",
        "                            output_units]) == units)\n",
        "\n",
        "        assert 'p_recurr' not in kwargs.keys(), 'p_recurr not supported in this method'\n",
        "        assert recurrent_kernel.shape == recurrent_nontrainable_boolean_mask.shape, \"Boolean mask and recurrent kernel shape mis-match\"\n",
        "        assert tf.math.count_nonzero(tf.boolean_mask(recurrent_kernel, recurrent_nontrainable_boolean_mask)).numpy() == 0, \"Invalid boolean mask\"  \n",
        "\n",
        "        self = cls(units=units, output_size=output_size, p_recurr = None, **kwargs)\n",
        "\n",
        "        self.recurrent_nontrainable_boolean_mask = tf.convert_to_tensor(recurrent_nontrainable_boolean_mask)\n",
        "        \n",
        "        self.initialize_input_kernel(input_shape, input_kernel)\n",
        "        self.initialize_recurrent_kernel(recurrent_kernel)\n",
        "        self.initialize_output_kernel(output_kernel)\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "        return self\n",
        "    "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmhznhB_1Xue",
        "outputId": "4e7c137c-b09b-4d6c-c99e-1d34ee83b2e3"
      },
      "source": [
        "n = 25\n",
        "m = 1\n",
        "\n",
        "myesn2 = NoFeedbackESN( dtdivtau= 0.1, units = n, output_size = m, activation = 'tanh', seed = 1)\n",
        "model2 = FORCEModel(myesn2, return_sequences=True)  \n",
        "model2.compile(metrics=[\"mae\"] , run_eagerly = False)\n",
        "\n",
        "history2 = model2.fit(x=np.zeros((300,1)).astype(np.float32), y= np.random.normal(size = (300,1)).astype(np.float32) , epochs = 20)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1/1 [==============================] - 141s 141s/step - mae: 0.7487\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 91ms/step - mae: 0.7363\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 113ms/step - mae: 0.7357\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 97ms/step - mae: 0.7354\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 103ms/step - mae: 0.7352\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 95ms/step - mae: 0.7351\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 91ms/step - mae: 0.7350\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 97ms/step - mae: 0.7350\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 94ms/step - mae: 0.7349\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 100ms/step - mae: 0.7349\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 98ms/step - mae: 0.7348\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 95ms/step - mae: 0.7348\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 106ms/step - mae: 0.7348\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 94ms/step - mae: 0.7348\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 93ms/step - mae: 0.7348\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 93ms/step - mae: 0.7347\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 97ms/step - mae: 0.7347\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 102ms/step - mae: 0.7347\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 98ms/step - mae: 0.7347\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 94ms/step - mae: 0.7347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blAXDrfTN2x5",
        "outputId": "a07f19c3-4d1f-409d-9986-a872915f5a81"
      },
      "source": [
        "for i in range(model2.P_GG.shape[0]):\n",
        "    print(tf.math.reduce_sum((model2.P_GG[0] - model2.P_GG[i])**2))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n"
          ]
        }
      ]
    }
  ]
}
